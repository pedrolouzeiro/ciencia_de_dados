{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1txgXQKv0N6"
   },
   "source": [
    "# Entendendo o Problema\n",
    "\n",
    "Uma imobiliária situada na cidade do Rio de Janeiro está enfrentando dificuldades para alugar e vender imóveis. Durante uma pesquisa sobre como empresas semelhantes operam no mercado, a imobiliária identificou que esse problema pode estar relacionado aos valores dos imóveis e às recomendações feitas aos clientes.\n",
    "\n",
    "## Objetivo do Projeto\n",
    "\n",
    "Esse projeto tem como objetivo solucionar os desafios enfrentados pela imobiliária por meio de três etapas principais:\n",
    "\n",
    "1. **Análise e Tratamento dos Dados**  \n",
    "   - Ler e realizar o tratamento do histórico de preços de imóveis no Rio de Janeiro.  \n",
    "   - Identificar padrões e tendências que podem estar influenciando o mercado.  \n",
    "\n",
    "2. **Construção de um Modelo de Regressão**  \n",
    "   - Desenvolver um modelo de regressão para precificar imóveis de forma precisa e competitiva.  \n",
    "\n",
    "3. **Criação de um Sistema Recomendador**  \n",
    "   - Construir um recomendador de imóveis para oferecer sugestões personalizadas aos clientes com base em suas preferências e perfil.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY7uQ6XatJvo"
   },
   "source": [
    "# Extraindo base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCoA-QfkElNU",
    "outputId": "0ec6cc3a-fc64-4222-de72-2f6416f15ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-28 19:46:36--  https://caelum-online-public.s3.amazonaws.com/challenge-spark/semana-1.zip\n",
      "Resolving caelum-online-public.s3.amazonaws.com (caelum-online-public.s3.amazonaws.com)... 52.216.218.73, 3.5.30.39, 52.216.214.137, ...\n",
      "Connecting to caelum-online-public.s3.amazonaws.com (caelum-online-public.s3.amazonaws.com)|52.216.218.73|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18975214 (18M) [application/zip]\n",
      "Saving to: ‘semana-1.zip’\n",
      "\n",
      "semana-1.zip        100%[===================>]  18.10M  6.81MB/s    in 2.7s    \n",
      "\n",
      "2024-11-28 19:46:40 (6.81 MB/s) - ‘semana-1.zip’ saved [18975214/18975214]\n",
      "\n",
      "Archive:  semana-1.zip\n",
      "  inflating: dataset_bruto.json      \n"
     ]
    }
   ],
   "source": [
    "!wget https://caelum-online-public.s3.amazonaws.com/challenge-spark/semana-1.zip && unzip semana-1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-UNG2-MQZr1"
   },
   "source": [
    "# Instalando as dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CGwZt8VFFiTM"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dUjripCPWmxF"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pfo8w7XtUpx"
   },
   "source": [
    "# Inicializando Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-gF2DI8yQrDm"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .master('local[*]') \\\n",
    "  .appName(\"previsao_imoveis\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fB0JgqyHREv6"
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('/content/dataset_bruto.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmW-Y_gfUhA5",
    "outputId": "18d5962d-965c-4031-bbba-e5690caff2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             anuncio|             imagens|             usuario|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{0, [], [16], [0]...|[{39d6282a-71f3-4...|{9d44563d-3405-4e...|\n",
      "|{0, [], [14], [0]...|[{23d2b3ab-45b0-4...|{36245be7-70fe-40...|\n",
      "|{0, [1026], [1026...|[{1da65baa-368b-4...|{9dc415d8-1397-4d...|\n",
      "|{0, [120], [120],...|[{79b542c6-49b4-4...|{9911a2df-f299-4a...|\n",
      "|{0, [3], [3], [0]...|[{e2bc497b-6510-4...|{240a7aab-12e5-40...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxQ-9sGPUsuf"
   },
   "source": [
    "Para nossa análise, apenas as informações do campo \"anuncio\" serão relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "etEyUZOgRNyb"
   },
   "outputs": [],
   "source": [
    "analise = df.select('anuncio.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7Tp0GJgMUjE"
   },
   "source": [
    "# Tratamento e preparando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx6g07ZfHBK-"
   },
   "source": [
    "Antes de criar a função, analisei e entendi que a base de dados necessitava do seguinte tratamento:\n",
    "\n",
    "1.   Transformação dos dados das colunas \"quartos\", \"suites\", \"banheiros\", \"vaga\", \"area_total\" e \"area_util\" de listas para inteiros.\n",
    "2.   Transformação dos dados da coluna \"valores\" em colunas separadas para melhor compreensão.\n",
    "3.   A equipe solicitou que apenas as informações sobre bairro e zona da cidade fossem extraídas, como se trata de um estudo sobre o preço de venda dos imóveis, será utilizada apenas as informações do tipo VENDA, também foi solicitado que fizéssemos alguns filtros nas colunas tipo_uso, tipo_unidade e tipo_anuncio da nossa base de dados: tipo_uso: Residencial, tipo_unidade: Apartamento, tipo_anuncio: Usado e deveria apagar a coluna \"area_total\" pois tem as mesmas informações de \"area_util\" e mais valores ausentes.\n",
    "4. Tratamento coluna \"características\" e se a lista for vazia retornar 'nao informado'.\n",
    "5. Transformação dados NA em 0.\n",
    "6. Converter o tipo de colunas numéricas, como \"andar\", \"banheiros\", \"suites\" e \"quartos\" para o tipo inteiro e converter as colunas \"area_util\", \"condominio\", \"iptu\" e \"valor\" para o tipo double.\n",
    "7. Transformar variáveis categóricas em binárias.\n",
    "8. Um parametro para diferenciar se o tratamento é para o modelo de regressão ou para o recomendador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mdyI2D57ZqNJ"
   },
   "outputs": [],
   "source": [
    "def tratar_dados(df, modelo):\n",
    "    # 1. Converte colunas de listas para inteiros\n",
    "    colunas_a_transformar = ['area_total', 'quartos', 'suites', 'banheiros', 'vaga', 'area_util']\n",
    "    for coluna in colunas_a_transformar:\n",
    "        df = df.withColumn(coluna, f.element_at(f.col(coluna), 1).cast(IntegerType()))\n",
    "\n",
    "    # 2. Explode a coluna \"valores\" e extrai informações relevantes\n",
    "    df = df.withColumn('valores', f.explode('valores')).select(\n",
    "        '*',\n",
    "        f.col('valores.condominio').alias('condominio'),\n",
    "        f.col('valores.iptu').alias('iptu'),\n",
    "        f.col('valores.tipo').alias('tipo'),\n",
    "        f.col('valores.valor').alias('valor'),\n",
    "        f.col('endereco.bairro').alias('bairro'),\n",
    "        f.col('endereco.zona').alias('zona')\n",
    "    )\n",
    "\n",
    "    # 3. Filtra os dados conforme os critérios\n",
    "    df = df.filter(\n",
    "        (f.col('tipo_uso') == 'Residencial') &\n",
    "        (f.col('tipo_anuncio') == 'Usado') &\n",
    "        (f.col('tipo_unidade') == 'Apartamento') &\n",
    "        (f.col('tipo') == 'Venda')\n",
    "    ).drop('valores', 'endereco', 'tipo_uso', 'tipo_unidade', 'tipo_anuncio', 'area_total', 'tipo')\n",
    "\n",
    "    # 4. Trata a coluna \"caracteristicas\"\n",
    "    df = df.withColumn(\n",
    "        'caracteristicas',\n",
    "        f.when(f.size(f.col('caracteristicas')) == 0, f.array(f.lit('nao informado'))).otherwise(f.col('caracteristicas'))\n",
    "    )\n",
    "\n",
    "    # 5. Preenche valores nulos com 0\n",
    "    df = df.na.fill({'iptu': 0, 'quartos': 0, 'suites': 0, 'banheiros': 0, 'vaga': 0, 'condominio': 0})\n",
    "\n",
    "    # 6. Converte tipos de colunas\n",
    "    colunas_para_conversao = {\n",
    "        'andar': IntegerType(),\n",
    "        'banheiros': IntegerType(),\n",
    "        'suites': IntegerType(),\n",
    "        'quartos': IntegerType(),\n",
    "        'area_util': DoubleType(),\n",
    "        'condominio': DoubleType(),\n",
    "        'iptu': DoubleType(),\n",
    "        'valor': DoubleType()\n",
    "    }\n",
    "    for coluna, tipo in colunas_para_conversao.items():\n",
    "        df = df.withColumn(coluna, f.col(coluna).cast(tipo))\n",
    "\n",
    "    # 7. Cria colunas binárias para características\n",
    "    caracteristicas = [\n",
    "        'Academia', 'Churrasqueira', 'Playground', 'Condomínio fechado',\n",
    "        'Portão eletrônico', 'Portaria 24h', 'Salão de festas',\n",
    "        'Piscina', 'Animais permitidos', 'Elevador'\n",
    "    ]\n",
    "    for caracteristica in caracteristicas:\n",
    "        df = df.withColumn(\n",
    "            f\"caracteristica_{caracteristica.replace(' ', '_')}\",\n",
    "            f.array_contains(f.col('caracteristicas'), caracteristica).cast(\"int\")\n",
    "        )\n",
    "\n",
    "    # Cria colunas binárias para zonas\n",
    "    zonas = ['Zona Norte', 'Zona Oeste', 'Zona Central', 'Zona Sul']\n",
    "    for zona in zonas:\n",
    "        df = df.withColumn(\n",
    "            f\"{zona.replace(' ', '_')}\",\n",
    "            (f.col('zona') == zona).cast(\"int\")\n",
    "        )\n",
    "\n",
    "    # 8. Remove colunas desnecessárias de acordo com o modelo\n",
    "    colunas_a_remover = ['caracteristicas', 'zona']\n",
    "    if modelo != 'rec':\n",
    "        colunas_a_remover.extend(['id', 'bairro'])\n",
    "    df = df.drop(*colunas_a_remover)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRwvn735MjpG"
   },
   "source": [
    "A conversão de tipos de colunas pode parecer redundante em alguns trechos, mas isso acontece porque há duas etapas distintas no processo, cada uma com uma motivação diferente:\n",
    "\n",
    "**1. Conversão inicial de colunas de listas para inteiros (`f.element_at`)**\n",
    "\n",
    "Na etapa inicial, algumas colunas como `\"area_total\"`, `\"quartos\"`, etc., estão representadas como listas (ou arrays) e precisam ser extraídas. O método `f.element_at(f.col(coluna), 1)` pega o primeiro elemento da lista e o converte para um tipo numérico (neste caso, `IntegerType`).\n",
    "\n",
    "Isso é necessário porque:\n",
    "- Esses valores chegam como listas devido à forma como os dados são estruturados originalmente (possivelmente em JSON ou outro formato de aninhamento).\n",
    "- Apenas extrair o valor do array não garante que o tipo da coluna seja o desejado, já que Spark pode inferir um tipo diferente.\n",
    "\n",
    "**2. Conversão geral de tipos de colunas para consistência**\n",
    "\n",
    "Mais tarde, existe uma conversão explícita de várias colunas (como `\"andar\"`, `\"quartos\"`, `\"area_util\"`, etc.) para tipos como `IntegerType` ou `DoubleType`.\n",
    "\n",
    "Essa conversão ocorre por dois motivos:\n",
    "1. **Correção de inconsistências nos dados**: Mesmo após o tratamento inicial, algumas colunas podem ter valores nulos ou tipos inesperados devido a transformações anteriores ou ao carregamento dos dados.\n",
    "2. **Normalização do tipo para o modelo ou análise**: Certos algoritmos de machine learning ou análises requerem tipos específicos (ex.: números inteiros ou de ponto flutuante). Essa conversão garante que os dados estejam no formato correto.\n",
    "\n",
    "**Por que não fazer tudo de uma vez?**\n",
    "\n",
    "Não é possível consolidar essas etapas em uma única conversão porque:\n",
    "- A primeira conversão (`f.element_at`) transforma dados do tipo lista/array em valores simples. Isso deve ser feito antes de qualquer conversão de tipo final.\n",
    "- A segunda conversão atua em colunas que já passaram por diversos tratamentos, incluindo preenchimento de valores nulos e filtragem.\n",
    "\n",
    "Ao final, cada conversão atende a uma necessidade específica e é aplicada no momento adequado. Se o dado já viesse normalizado, a primeira conversão seria desnecessária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9A_NVD4rZ1YI"
   },
   "outputs": [],
   "source": [
    "modelo = tratar_dados(analise, 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDnYUlF-m17Q",
    "outputId": "2dd230a0-153a-414b-930a-7b987e299805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------+-------+\n",
      "|features                                                                                       |valor  |\n",
      "+-----------------------------------------------------------------------------------------------+-------+\n",
      "|[3.0,43.0,1.0,2.0,0.0,1.0,245.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0]   |15000.0|\n",
      "|(22,[0,1,2,3,5,9,10,11,12,13,14,16,19],[2.0,42.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |15000.0|\n",
      "|(22,[0,1,2,3,5,9,10,11,12,13,14,19],[1.0,41.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])        |20000.0|\n",
      "|[3.0,43.0,1.0,2.0,0.0,0.0,285.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0]   |20000.0|\n",
      "|[2.0,43.0,1.0,2.0,0.0,1.0,245.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0]   |15000.0|\n",
      "|[3.0,43.0,1.0,2.0,0.0,0.0,285.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0]   |20000.0|\n",
      "|[3.0,43.0,1.0,2.0,0.0,1.0,250.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0]   |15000.0|\n",
      "|(22,[0,1,2,3,5,6,9,10,11,12,14,16,19],[3.0,43.0,1.0,2.0,1.0,245.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|15000.0|\n",
      "|[3.0,43.0,1.0,2.0,0.0,1.0,245.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0]   |15000.0|\n",
      "|[4.0,43.0,1.0,2.0,0.0,1.0,240.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0]   |17999.0|\n",
      "|(22,[1,2,3,5,11,19],[60.0,1.0,2.0,1.0,1.0,1.0])                                                |15000.0|\n",
      "|(22,[0,1,2,3,5,6,9,10,11,12,13,14,19],[1.0,43.0,1.0,2.0,1.0,240.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|19999.0|\n",
      "|(22,[1,2,3,6,9,10,11,12,13,14,16,19],[43.0,1.0,2.0,245.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])     |19999.0|\n",
      "|[2.0,43.0,1.0,2.0,0.0,2.0,290.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0]   |25000.0|\n",
      "|(22,[0,1,2,3,6,10,11,12,13,14,16,19],[3.0,43.0,1.0,2.0,285.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])     |29999.0|\n",
      "|[4.0,55.0,2.0,2.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0]     |30000.0|\n",
      "|(22,[1,2,3,5,9,10,11,14,15,19],[45.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                     |30000.0|\n",
      "|(22,[0,1,2,3,6,11,12,13,14,16,19],[5.0,42.0,1.0,2.0,280.0,1.0,1.0,1.0,1.0,1.0,1.0])            |25000.0|\n",
      "|[1.0,43.0,1.0,2.0,0.0,1.0,280.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0]   |27999.0|\n",
      "|[3.0,55.0,1.0,2.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0]     |26000.0|\n",
      "+-----------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Identificação das Features\n",
    "features_col = [col for col in modelo.columns if col != 'valor']\n",
    "\n",
    "# 2. Assembler\n",
    "vector_assembler = VectorAssembler(inputCols=features_col, outputCol=\"features\")\n",
    "\n",
    "# 3. Transformação\n",
    "modelo_vectorizado = vector_assembler.transform(modelo)\n",
    "\n",
    "# 4. Seleção das Colunas Relevantes\n",
    "modelo_vectorizado = modelo_vectorizado.select(\"features\", \"valor\")\n",
    "\n",
    "# 5. Exibir o DataFrame vetorizado\n",
    "modelo_vectorizado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4CTRSO2OaS4"
   },
   "source": [
    "1. **Identificação das Features**:\n",
    "   - As colunas do DataFrame são analisadas, e todas as colunas exceto a variável `valor` (o alvo da previsão) são selecionadas para formar a lista de features. Essas colunas representam as variáveis preditoras que serão usadas no modelo.\n",
    "\n",
    "2. **VectorAssembler**:\n",
    "   - Um transformador do Spark que combina múltiplas colunas de entrada em uma única coluna chamada `features`. Essa coluna contém um vetor que representa todos os valores das variáveis preditoras em cada linha do DataFrame.\n",
    "\n",
    "3. **Transformação**:\n",
    "   - O `VectorAssembler` é aplicado ao DataFrame, criando a nova coluna `features`. Este vetor de preditores será usado diretamente pelos algoritmos de machine learning do Spark.\n",
    "\n",
    "4. **Seleção das Colunas Relevantes**:\n",
    "   - Após a vetorização, apenas duas colunas são mantidas:\n",
    "     - `features`: o vetor das variáveis preditoras.\n",
    "     - `valor`: a variável-alvo (target).\n",
    "\n",
    "5. **Visualização**:\n",
    "   - O DataFrame resultante é exibido para inspecionar os vetores de features e seus valores associados no target.\n",
    "\n",
    "Essa etapa organiza e prepara os dados no formato esperado pela maioria dos algoritmos de machine learning do Spark, otimizando o pipeline de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_g5MhS9zFcT",
    "outputId": "709ccec5-d0f8-46e9-97e9-f60b29d75c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros em treino: 53107\n",
      "Número de registros em teste: 13455\n"
     ]
    }
   ],
   "source": [
    "# 1. Separação dos dados em treino (80%) e teste (20%)\n",
    "treino, teste = modelo_vectorizado.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 2. Exibir a contagem de registros em cada conjunto\n",
    "print(f\"Número de registros em treino: {treino.count()}\")\n",
    "print(f\"Número de registros em teste: {teste.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53hPARIZOyd0"
   },
   "source": [
    "1. **Separação dos Dados**:\n",
    "   - O método `randomSplit` do PySpark é usado para dividir os dados em dois subconjuntos: **treino** e **teste**.\n",
    "   - O argumento `[0.8, 0.2]` especifica as proporções de divisão:\n",
    "     - 80% dos dados vão para o conjunto de **treino** (usado para ajustar o modelo).\n",
    "     - 20% dos dados vão para o conjunto de **teste** (usado para avaliar o desempenho do modelo).\n",
    "   - O parâmetro `seed=42` garante que a divisão seja **reprodutível** (a mesma divisão será gerada sempre que o código for executado com os mesmos dados).\n",
    "\n",
    "2. **Contagem de Registros**:\n",
    "   - Após a separação, os métodos `treino.count()` e `teste.count()` contam o número de registros em cada conjunto.\n",
    "   - Essas contagens ajudam a verificar se os dados foram divididos nas proporções corretas.\n",
    "\n",
    "\n",
    "**Importância dessa etapa:**\n",
    "- A divisão em treino e teste é um **passo fundamental** no processo de aprendizado de máquina:\n",
    "  - **Treino**: O modelo aprende os padrões dos dados.\n",
    "  - **Teste**: O modelo é avaliado em dados que ele não viu antes, permitindo medir sua capacidade de generalizar.\n",
    "- Garantir proporções balanceadas e reprodutibilidade é essencial para obter uma avaliação confiável do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laf88LSN8n99"
   },
   "source": [
    "# Criação do Modelo Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YOMcXlVjPI-",
    "outputId": "2baefbc5-b0c7-4196-c95e-37ad629ada3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Mais Importantes:\n",
      "area_util: 0.297353318864422\n",
      "condominio: 0.24138059616461077\n",
      "quartos: 0.1340032579632543\n",
      "vaga: 0.07370743804995758\n",
      "banheiros: 0.06666138273769011\n",
      "Zona_Sul: 0.0469779409281179\n",
      "suites: 0.04029684752861563\n",
      "iptu: 0.03885480793047876\n",
      "Zona_Oeste: 0.015094482370555187\n",
      "andar: 0.01173371684217761\n",
      "Zona_Norte: 0.011665175116091175\n",
      "caracteristica_Piscina: 0.0030655559685234335\n",
      "caracteristica_Portaria_24h: 0.002977592127417346\n",
      "caracteristica_Academia: 0.0025648322786988044\n",
      "caracteristica_Churrasqueira: 0.00243103676839295\n",
      "caracteristica_Playground: 0.0022303839169130466\n",
      "caracteristica_Animais_permitidos: 0.0020873561725031522\n",
      "caracteristica_Portão_eletrônico: 0.0018076427956220556\n",
      "caracteristica_Salão_de_festas: 0.0017666316616361689\n",
      "caracteristica_Elevador: 0.0016473692963103934\n",
      "caracteristica_Condomínio_fechado: 0.0015853745669736265\n",
      "Zona_Central: 0.00010725995103795895\n"
     ]
    }
   ],
   "source": [
    "# 1. Criação do modelo de regressão com Random Forest\n",
    "rfr = RandomForestRegressor(seed=101, maxDepth=10, numTrees=20, featuresCol='features', labelCol='valor')\n",
    "\n",
    "# 2. Criação do avaliador de regressão\n",
    "evaluator = RegressionEvaluator(labelCol='valor', predictionCol='prediction')\n",
    "\n",
    "# 3. Treinamento do modelo\n",
    "modelo_rfr = rfr.fit(treino)\n",
    "\n",
    "# 4. Realização de previsões\n",
    "previsoes_rfr_test = modelo_rfr.transform(teste)\n",
    "\n",
    "importancias = modelo_rfr.featureImportances.toArray()\n",
    "\n",
    "# 6. Associar com os nomes das features\n",
    "features_importancia = list(zip(features_col, importancias))\n",
    "\n",
    "# 7. Ordenar pela importância\n",
    "ordem_importancia = sorted(features_importancia, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 8. Exibição das features mais importantes\n",
    "print('Features Mais Importantes:')\n",
    "for feature, importancias in ordem_importancia:\n",
    "    print(f\"{feature}: {importancias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25do474lQyIM"
   },
   "source": [
    "1. **Criação do modelo de regressão com Random Forest**:\n",
    "   - O **RandomForestRegressor** é um modelo de machine learning utilizado para tarefas de regressão. Ele cria várias árvores de decisão e as combina para fazer previsões mais robustas.\n",
    "   - O modelo foi configurado com parâmetros como o número de árvores na floresta (`numTrees=20`), a profundidade máxima das árvores (`maxDepth=10`), e as colunas que contêm as variáveis preditoras e o valor alvo (`featuresCol` e `labelCol`).\n",
    "\n",
    "2. **Criação do avaliador de regressão**:\n",
    "   - O **RegressionEvaluator** é usado para avaliar a qualidade do modelo. Ele compara as previsões feitas pelo modelo com os valores reais (coluna `valor`) e pode calcular métricas como o erro quadrático médio (RMSE) ou o R².\n",
    "   - O avaliador foi configurado para usar a coluna `valor` como a variável-alvo e a coluna `prediction` como a variável com as previsões feitas pelo modelo.\n",
    "\n",
    "3. **Treinamento do modelo**:\n",
    "   - O modelo (`rfr`) foi treinado usando o conjunto de dados de treinamento (`treino`) com o método `fit()`. Esse processo ajusta o modelo aos dados, aprendendo as relações entre as features e o valor a ser predito.\n",
    "\n",
    "4. **Realização de previsões**:\n",
    "   - Após o treinamento, o modelo fez previsões no conjunto de teste (`teste`) com o método `transform()`. As previsões geradas pelo modelo são armazenadas na coluna `prediction` do DataFrame resultante.\n",
    "\n",
    "5. **Extração das importâncias das features**:\n",
    "   - Uma vez que o modelo está treinado, é possível acessar a importância de cada feature usando o atributo `featureImportances`. Isso retorna um vetor que indica o peso de cada feature na decisão das árvores de regressão do Random Forest.\n",
    "\n",
    "6. **Associação das importâncias com os nomes das features**:\n",
    "   - As importâncias das features são combinadas com seus respectivos nomes usando a função `zip()`, formando uma lista de tuplas onde cada tupla contém o nome da feature e a sua importância correspondente.\n",
    "\n",
    "7. **Ordenação das features pela importância**:\n",
    "   - A lista de features e suas importâncias é ordenada de forma decrescente para que as features mais importantes fiquem no topo.\n",
    "\n",
    "8. **Exibição das features mais importantes**:\n",
    "   - Finalmente, o código percorre a lista ordenada de features e exibe o nome de cada uma junto com seu valor de importância. Isso permite visualizar quais variáveis têm maior influência na previsão do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O72woFN8sHt"
   },
   "source": [
    "# Otimização e Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-gzBb1TpZC1",
    "outputId": "ba825b18-51e0-4515-f7c3-6e4570bb0db1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem Cross Validation\n",
      "==============================\n",
      "R²: 0.828595\n",
      "RMSE: 601843.056074\n",
      "==============================\n",
      "==============================\n",
      "Com Cross Validation\n",
      "==============================\n",
      "R²: 0.841141\n",
      "RMSE: 579399.624267\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(featuresCol='features', labelCol='valor')\n",
    "# 1. Construção da Grade de Parâmetros para Busca (Grid Search)\n",
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(rfr.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rfr.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rfr.maxBins, [10, 32, 45]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='valor', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# 2. Treinamento do Modelo com Validação Cruzada\n",
    "rfr_cv = CrossValidator(\n",
    "    estimator=rfr,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "modelo_rfr_cv = rfr_cv.fit(treino)\n",
    "previsoes_rfr_cv_teste = modelo_rfr_cv.transform(teste)\n",
    "\n",
    "# 3. Avaliação do Modelo sem Validação Cruzada e com Validação Cruzada\n",
    "print('Sem Cross Validation')\n",
    "print('='*30)\n",
    "print('R²: %f' % evaluator.evaluate(previsoes_rfr_test, {evaluator.metricName: 'r2'}))\n",
    "print('RMSE: %f' % evaluator.evaluate(previsoes_rfr_test, {evaluator.metricName: 'rmse'}))\n",
    "print('='*30)\n",
    "print('='*30)\n",
    "print('Com Cross Validation')\n",
    "print('='*30)\n",
    "print('R²: %f' % evaluator.evaluate(previsoes_rfr_cv_teste, {evaluator.metricName: 'r2'}))\n",
    "print('RMSE: %f' % evaluator.evaluate(previsoes_rfr_cv_teste, {evaluator.metricName: 'rmse'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZLfNngtTqKo"
   },
   "source": [
    "1. **Construção da Grade de Parâmetros para Busca (Grid Search)**:\n",
    "   - **ParamGridBuilder** cria uma grade de parâmetros a ser testada. O código define diferentes valores para três parâmetros do modelo:\n",
    "     - `numTrees`: número de árvores na floresta (opções: 10, 20, 30).\n",
    "     - `maxDepth`: profundidade máxima das árvores (opções: 5, 10).\n",
    "     - `maxBins`: número máximo de bins (intervalos) usados para dividir os dados (opções: 10, 32, 45).\n",
    "   - O objetivo é testar essas combinações de parâmetros para encontrar a configuração que gera o melhor desempenho no modelo.\n",
    "\n",
    "2. **Validação Cruzada (Cross-Validation)**:\n",
    "   - **CrossValidator** é utilizado para realizar validação cruzada no modelo. Ele divide o conjunto de dados de treino em 3 partes (folds), treina o modelo 3 vezes (uma para cada divisão) e testa em cada uma das partes separadas.\n",
    "   - A validação cruzada ajuda a reduzir o risco de overfitting, fornecendo uma avaliação mais robusta do modelo.\n",
    "\n",
    "3. **Avaliação do Modelo sem Validação Cruzada e com Validação Cruzada**:\n",
    "   - O modelo de Random Forest, sem validação cruzada, é avaliado com base no **R²** (coeficiente de determinação) e **RMSE** para as previsões feitas no conjunto de teste e o modelo treinado com validação cruzada é avaliado com as mesmas métricas (**R²** e **RMSE**) nas previsões feitas no conjunto de teste, permitindo comparar como o modelo se comporta com e sem a validação cruzada.\n",
    "\n",
    "Em resumo, essa etapa treina e avalia um modelo de Random Forest de duas maneiras: sem validação cruzada e com validação cruzada, e compara os resultados usando as métricas de avaliação de regressão.\n",
    "\n",
    "\n",
    "### Sem Validação Cruzada\n",
    "- **R²: 0.828595**: O coeficiente de determinação (R²) indica que aproximadamente 82.86% da variância nos dados de saída pode ser explicada pelo modelo. Isso sugere que o modelo tem um bom ajuste aos dados.\n",
    "- **RMSE: 601843.056074**: O erro quadrático médio (RMSE) mede a diferença média entre os valores previstos pelo modelo e os valores reais. Um RMSE de 601843.056074 indica que, em média, as previsões do modelo estão a cerca de 601843 unidades do valor real.\n",
    "\n",
    "### Com Validação Cruzada\n",
    "- **R²: 0.833689**: Com a validação cruzada, o R² aumentou para 83.37%, indicando uma ligeira melhoria na capacidade do modelo de explicar a variância nos dados.\n",
    "- **RMSE: 592832.342916**: O RMSE diminuiu para 592832.342916, o que significa que as previsões do modelo estão, em média, mais próximas dos valores reais em comparação com o modelo sem validação cruzada.\n",
    "\n",
    "### Interpretação dos Resultados\n",
    "A validação cruzada ajudou a melhorar o desempenho do modelo, resultando em um R² ligeiramente maior e um RMSE menor. Isso sugere que o modelo com validação cruzada é mais robusto e tem melhor capacidade preditiva, reduzindo o risco de overfitting e fornecendo previsões mais precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOeVdCXSy6BR"
   },
   "source": [
    "# Preparação para sistema de recomendação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqX2v-7q-g-F"
   },
   "source": [
    "É importante padronizarmos os dados para conseguirmos utilizar o PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jZCcM0F4-LQA"
   },
   "outputs": [],
   "source": [
    "# 1. Criação e Treinamento do Scaler\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=False)\n",
    "scaler_modelo = scaler.fit(modelo_vectorizado)\n",
    "\n",
    "# 2. Aplicação do Scaler nos Dados\n",
    "modelo_padronizado = scaler_modelo.transform(modelo_vectorizado)\n",
    "\n",
    "# 3. Seleção das Colunas Relevantes\n",
    "modelo_padronizado = modelo_padronizado.select('scaledFeatures', 'valor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqmF0CjCXN4d"
   },
   "source": [
    "1. **Criação e Treinamento do Scaler**:\n",
    "   - O **StandardScaler** é uma ferramenta para **padronizar** as features de um conjunto de dados. Ele ajusta as variáveis para que tenham um desvio padrão de 1, o que é importante quando variáveis têm escalas diferentes. A padronização ajuda a garantir que todas as variáveis contribuam igualmente para o modelo, evitando que variáveis com maior escala dominem o processo de aprendizado.\n",
    "   - No código, a transformação é aplicada à coluna `features` e os resultados escalados são armazenados na coluna `scaledFeatures`.\n",
    "   - O método **fit()** calcula os parâmetros necessários para a padronização (como o desvio padrão) com base nas variáveis presentes na coluna `features` do DataFrame `modelo_vectorizado`. Este é um passo de aprendizado, onde o modelo calcula como as variáveis devem ser transformadas.\n",
    "\n",
    "2. **Aplicação do Scaler nos Dados**:\n",
    "   - O método **transform()** aplica a transformação calculada pelo **fit()** aos dados de entrada, gerando uma nova coluna (`scaledFeatures`) com as features já escaladas. As variáveis originais na coluna `features` são transformadas para a mesma escala, o que torna os dados mais consistentes e equilibrados para o modelo de aprendizado.\n",
    "\n",
    "3. **Seleção das Colunas Relevantes**:\n",
    "   - Após a transformação, o código seleciona apenas as colunas `scaledFeatures` e `valor`, que representam, respectivamente, as variáveis preditoras escaladas e a variável alvo (valor). O restante das colunas é descartado para focar apenas nas variáveis relevantes para o modelo.\n",
    "\n",
    "**Importância dessa Etapa:**\n",
    "- A padronização é um pré-requisito para garantir que o PCA opere de maneira justa e equilibrada entre as diferentes variáveis, resultando em uma melhor redução de dimensionalidade e em componentes principais que refletem a verdadeira variação dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxxq03GV_Q06"
   },
   "source": [
    "Para conseguirmos criar nosso modelo de recomendação, precisamos reduzir a dimensão dos nossos dados. Para fazermos isso, podemos utilizar a técnica chamada PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ge24BRHs_2fX"
   },
   "outputs": [],
   "source": [
    "# 1. Configuração do PCA\n",
    "# Número de K = número de features\n",
    "pca = PCA(k=22, inputCol='scaledFeatures', outputCol='pca_features')\n",
    "\n",
    "# 2. Ajuste do Modelo PCA\n",
    "modelo = pca.fit(modelo_padronizado)\n",
    "\n",
    "# 3. Aplicação do Modelo PCA nos Dados\n",
    "modelo_pca = modelo.transform(modelo_padronizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55SDOsKLX8gp"
   },
   "source": [
    "1. **Configuração do PCA**:\n",
    "   - **PCA (Principal Component Analysis)** é uma técnica de redução de dimensionalidade que transforma um conjunto de variáveis correlacionadas em um conjunto de variáveis não correlacionadas, chamadas de componentes principais. O objetivo do PCA é preservar a maior parte da variação dos dados enquanto reduz o número de variáveis.\n",
    "   - O PCA é configurado para:\n",
    "     - **k=22**: Isso significa que o PCA reduzirá os dados originais para 22 componentes principais, ou seja, manterá 22 novas variáveis que representam a maior parte da variabilidade dos dados originais.\n",
    "     - **inputCol=\"scaledFeatures\"**: O PCA aplicará a transformação sobre as variáveis que já foram escaladas e armazenadas na coluna `scaledFeatures`.\n",
    "     - **outputCol=\"pca_features\"**: O resultado da transformação será armazenado na coluna `pca_features`, que conterá os 22 componentes principais calculados.\n",
    "\n",
    "2. **Ajuste do Modelo PCA**:\n",
    "   - O método **fit()** calcula os componentes principais com base nas **features escaladas** e ajusta o modelo PCA para os dados fornecidos. Esse modelo PCA agora está pronto para ser usado em dados de entrada e transformá-los nas novas variáveis (componentes principais).\n",
    "\n",
    "3. **Aplicação do Modelo PCA nos Dados**:\n",
    "   - O método transform() aplica a transformação PCA aos dados. O resultado, modelo_pca, contém os dados com as features originais reduzidas para os 22 componentes principais, que são armazenados na coluna pca_features.\n",
    "\n",
    "**Importância dessa Etapa**:\n",
    "- A transformação dos dados em **componentes principais** ajuda a eliminar **multicolinearidade** (correlação entre as variáveis), tornando os dados mais adequados para certos algoritmos de machine learning.\n",
    "- Reduzindo o número de features, o PCA também pode melhorar a **interpretação** dos dados, pois as componentes principais podem capturar os aspectos mais relevantes da variabilidade dos dados de uma maneira mais compacta e gerenciável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oEP0brkBve0",
    "outputId": "95fa77b9-f081-47bb-c5b3-391040d7348c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Cálculo da Variância Explicada Acumulada\n",
    "lista_valores = [sum(modelo.explainedVariance[0:i+1]) for i in range(22)]\n",
    "\n",
    "sum(np.array(lista_valores) <= 0.9) # 2. 90% é a taxa mínima de explicação que escolhi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3M_ifPD-ZK4R"
   },
   "source": [
    "1. **Cálculo da Variância Explicada Acumulada**:\n",
    "   Cria uma lista que armazena a **variância explicada acumulada**. A cada novo componente principal, a variância explicada é somada ao total acumulado, permitindo ver **quanto da variabilidade total dos dados** é capturado progressivamente pelos componentes principais.\n",
    "\n",
    "2. **Determinação do Número de Componentes que Explicam pelo Menos 90% da Variância**:\n",
    "   O último passo conta quantos componentes principais são necessários para explicar **90% da variância** dos dados. Isso ajuda a decidir quantos componentes manter para uma boa redução de dimensionalidade, garantindo que a maior parte da informação dos dados seja preservada ao reduzir o número de variáveis (componentes principais).\n",
    "\n",
    "**Importância dessa etapa**:\n",
    "- Esse resultado significa que, ao usar os primeiros 12 componentes, você consegue reduzir a dimensionalidade dos dados mantendo uma quantidade significativa de informação (90% da variabilidade dos dados), o que pode melhorar a eficiência computacional e reduzir o risco de overfitting, sem perder muito poder preditivo no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7KrWGijAZOV"
   },
   "source": [
    "Usando o método do cotovelo para descobrir o melhor número de K's na clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CvEhoiAEARD-"
   },
   "outputs": [],
   "source": [
    "# Inicializar uma lista para armazenar os valores de SSE\n",
    "sse_values = {}\n",
    "\n",
    "# Testar diferentes valores de K (número de clusters)\n",
    "for k in range(2, 51):  # de 2 até 50 clusters\n",
    "    kmeans = KMeans(k=k, seed=1, featuresCol='pca_features', predictionCol='cluster')\n",
    "    model = kmeans.fit(modelo_pca)\n",
    "\n",
    "    # Predições e SSE\n",
    "    predictions = model.transform(modelo_pca)\n",
    "    evaluator = ClusteringEvaluator(predictionCol='cluster', featuresCol='pca_features')\n",
    "    sse = evaluator.evaluate(predictions)  # Calcula a medida de erro (SSE)\n",
    "\n",
    "    sse_values[k] = sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBHKLJgDagEF"
   },
   "source": [
    "A **importância dessa etapa** é a seguinte:\n",
    "\n",
    "1. **Avaliação de Diferentes Quantidades de Clusters**:\n",
    "   O código testa diferentes valores de **`k`** (número de clusters) para o modelo K-means. Isso é importante porque o número de clusters ideal não é algo que se sabe de antemão e precisa ser determinado com base nos dados. A ideia é explorar vários valores de `k` para entender como o número de clusters afeta a qualidade do modelo.\n",
    "\n",
    "2. **Cálculo do SSE (Soma dos Erros Quadráticos)**:\n",
    "   A **Soma dos Erros Quadráticos (SSE)** é uma métrica usada para avaliar a qualidade do clustering. Ela mede a soma das distâncias quadradas entre os pontos de dados e os centros dos clusters. Quanto menor o SSE, melhor o modelo de clustering, pois significa que os pontos de dados estão mais próximos dos centros dos clusters atribuídos. A importância dessa métrica é fornecer uma indicação de quão bem o modelo está agrupando os dados. Ao comparar o SSE para diferentes valores de `k`, é possível identificar um número de clusters que minimize o erro, equilibrando a complexidade do modelo e a qualidade do agrupamento.\n",
    "\n",
    "3. **Exploração de Diferentes Valores de K (Número de Clusters)**:\n",
    "   O código percorre valores de **`k`** de 2 até 50 e calcula o SSE para cada um desses valores. Isso é crucial para entender como o modelo se comporta com diferentes números de clusters. A principal vantagem disso é que podemos buscar por um **número ótimo de clusters**, geralmente identificado pelo ponto de inflexão na curva do SSE. Esse ponto, conhecido como \"cotovelo\", indica onde adicionar mais clusters não resulta em uma melhoria significativa no modelo.\n",
    "\n",
    "4. **Armazenamento dos Resultados de SSE**:\n",
    "   Ao armazenar os valores de SSE para cada número de clusters `k`, o código fornece um histórico completo da avaliação do modelo, o que facilita a análise posterior. A partir desses resultados, podemos gerar uma **curva de SSE** versus o número de clusters e observar qual valor de `k` oferece o melhor equilíbrio entre precisão (baixo SSE) e complexidade (número de clusters). Isso ajuda na decisão de qual modelo de clustering utilizar, evitando tanto o subajuste (k muito baixo) quanto o sobreajuste (k muito alto).\n",
    "\n",
    "**Importância dessa etapa**:\n",
    "- Em resumo, essa etapa é essencial para **determinar o número ideal de clusters** para o problema de clustering e garantir que o modelo K-means seja ajustado corretamente, capturando a estrutura subjacente dos dados de maneira eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPZt3lMXBwHp",
    "outputId": "c09383dc-e600-4ffc-e864-9d9882749795"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.3895206818434673,\n",
       " 3: 0.3664486523831662,\n",
       " 4: 0.3020907243729316,\n",
       " 5: 0.3123389269670627,\n",
       " 6: 0.3014052889671764,\n",
       " 7: 0.26001675258937396,\n",
       " 8: 0.26724807089138825,\n",
       " 9: 0.3341900296915097,\n",
       " 10: 0.33271972591770227,\n",
       " 11: 0.31831104740506616,\n",
       " 12: 0.33232176550573017,\n",
       " 13: 0.35158795896493983,\n",
       " 14: 0.29938567551563333,\n",
       " 15: 0.3634638245555099,\n",
       " 16: 0.34165577113809414,\n",
       " 17: 0.3471505486629403,\n",
       " 18: 0.3337034616551103,\n",
       " 19: 0.3006307665446842,\n",
       " 20: 0.3079413347937255,\n",
       " 21: 0.319199921241598,\n",
       " 22: 0.32378311173650254,\n",
       " 23: 0.34885149442479607,\n",
       " 24: 0.2954915465802582,\n",
       " 25: 0.323715876569969,\n",
       " 26: 0.29044913293730296,\n",
       " 27: 0.30992555760292956,\n",
       " 28: 0.3243813352669043,\n",
       " 29: 0.3090923539518456,\n",
       " 30: 0.31446167341446163,\n",
       " 31: 0.255619095496541,\n",
       " 32: 0.2917938800102782,\n",
       " 33: 0.31510443823316653,\n",
       " 34: 0.318100479729942,\n",
       " 35: 0.31189712390681346,\n",
       " 36: 0.31209458374246096,\n",
       " 37: 0.30275981496343735,\n",
       " 38: 0.2774835259051263,\n",
       " 39: 0.3120397864340143,\n",
       " 40: 0.32351067493508506,\n",
       " 41: 0.3180485543603428,\n",
       " 42: 0.2971830966016679,\n",
       " 43: 0.28139591006843706,\n",
       " 44: 0.302087837231482,\n",
       " 45: 0.28899955538027394,\n",
       " 46: 0.3170187146350227,\n",
       " 47: 0.3229338838197324,\n",
       " 48: 0.3155730964216541,\n",
       " 49: 0.3094352009955571,\n",
       " 50: 0.31607932273404427}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpPXpGZVuJpr"
   },
   "source": [
    "K = 15 foi o melhor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yj6yrvPpEFbf"
   },
   "source": [
    "Utilizando Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Tc51l809ItJM"
   },
   "outputs": [],
   "source": [
    "dados = tratar_dados(df.select('anuncio.*'), 'rec')\n",
    "\n",
    "pipeline = Pipeline(stages=[VectorAssembler(inputCols=features_col, outputCol='features'), StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=False), PCA(k=12, inputCol='scaledFeatures', outputCol='pca_features'), KMeans(k=15, featuresCol='pca_features', predictionCol='cluster')])\n",
    "pipeline_modelo = pipeline.fit(dados)\n",
    "df_pronto = pipeline_modelo.transform(dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6ScJ5_sbfp1"
   },
   "source": [
    "Criação de pipeline com os melhores parametros encontrados, cada componente no pipeline tem uma função específica (Para reforçar):\n",
    "\n",
    "1. **VectorAssembler**: Prepara os dados, combinando várias colunas de características em um único vetor. Isso é essencial para transformar os dados em um formato adequado para os algoritmos de aprendizado de máquina, como KMeans.\n",
    "\n",
    "2. **StandardScaler**: Normaliza as características, garantindo que todas as variáveis tenham a mesma escala. Isso é importante porque muitos algoritmos, como o PCA e KMeans, são sensíveis à escala dos dados, e a padronização ajuda a melhorar a performance do modelo.\n",
    "\n",
    "3. **PCA (Principal Component Analysis)**: Reduz a dimensionalidade dos dados, mantendo as componentes mais significativas. Isso ajuda a diminuir a complexidade do modelo e melhora a eficiência computacional, especialmente em grandes conjuntos de dados.\n",
    "\n",
    "4. **KMeans**: Realiza o agrupamento dos dados em clusters, com base nas características transformadas. O modelo tenta identificar grupos semelhantes de dados, o que pode ser útil para várias análises, como segmentação de clientes ou identificação de padrões.\n",
    "\n",
    "**Importância dessa etapa**:\n",
    "- Ao unir essas etapas em um pipeline, o processo se torna mais eficiente e repetível, eliminando a necessidade de realizar cada transformação separadamente. Além disso, a utilização de um pipeline facilita ajustes futuros no modelo, já que todas as etapas são encapsuladas de forma organizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7zdWD6ujdAa"
   },
   "source": [
    "# Função para Sistema Recomendador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3h7dfq_gVXB1"
   },
   "outputs": [],
   "source": [
    "def recomenda_imovel(id, df):\n",
    "\n",
    "  # Função da distância euclidiana\n",
    "  def distancia(imovel, valor):\n",
    "    return euclidean(imovel, valor)\n",
    "\n",
    "  # 1. Identificação do cluster\n",
    "  cluster = df\\\n",
    "        .filter(df.id == id)\\\n",
    "        .select('cluster')\\\n",
    "        .collect()[0][0]\n",
    "\n",
    "  # 2. Filtragem de imóveis no mesmo cluster\n",
    "  imoveis_recomendados = df\\\n",
    "      .filter(df.cluster == cluster)\n",
    "\n",
    "  # 3. Obtendo as características PCA do imóvel procurado\n",
    "  imovel_procurado = imoveis_recomendados\\\n",
    "      .filter(imoveis_recomendados.id == id)\\\n",
    "      .select('pca_features')\\\n",
    "      .collect()[0][0]\n",
    "\n",
    "  # 4. Cálculo da distância euclidiana\n",
    "  distancia_udf = f.udf(lambda x: distancia(\n",
    "      imovel_procurado, x), FloatType())\n",
    "\n",
    "  # 5. Ordenação e filtragem dos imóveis\n",
    "  colunas_nao_utilizadas = [\n",
    "      'features', 'scaled_features', 'pca_features', 'cluster', 'distancia']\n",
    "\n",
    "  recomendacao = imoveis_recomendados\\\n",
    "      .withColumn('distancia', distancia_udf('pca_features'))\\\n",
    "      .filter(imoveis_recomendados.id != id)\\\n",
    "      .select([col for col in imoveis_recomendados.columns if col not in colunas_nao_utilizadas])\\\n",
    "      .orderBy('distancia')\n",
    "\n",
    "  # 6. Retorno das recomendações\n",
    "  return recomendacao.select('andar', 'area_util', 'quartos', 'suites', 'vaga', 'bairro', 'valor').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfEG2mWadb6n"
   },
   "source": [
    "A função **`recomenda_imovel`** tem como objetivo recomendar imóveis semelhantes a um imóvel específico com base em seu cluster e utilizando a **distância euclidiana** entre as características dos imóveis, após a aplicação do **PCA (Principal Component Analysis)** para redução de dimensionalidade. A função segue estas etapas:\n",
    "\n",
    "1. **Identificação do cluster**: A partir do ID do imóvel fornecido, a função obtém o cluster ao qual o imóvel pertence. Isso é feito para garantir que a recomendação seja feita dentro do mesmo grupo de imóveis, baseando-se em características semelhantes.\n",
    "\n",
    "2. **Filtragem de imóveis no mesmo cluster**: Após identificar o cluster, a função seleciona todos os imóveis que pertencem ao mesmo grupo, aumentando a chance de recomendação de imóveis realmente similares.\n",
    "\n",
    "3. **Obtendo as características PCA do imóvel procurado**: A função coleta as características PCA do imóvel que foi fornecido como entrada, que representam as variáveis mais significativas do imóvel no espaço de componentes principais.\n",
    "\n",
    "4. **Cálculo da distância euclidiana**: A função define uma métrica de distância (distância euclidiana) entre o imóvel procurado e outros imóveis, utilizando as características PCA. Essa distância é usada para avaliar a similaridade entre os imóveis.\n",
    "\n",
    "5. **Ordenação e filtragem dos imóveis**: Com base na distância calculada, a função ordena os imóveis do cluster, colocando os mais semelhantes ao imóvel procurado no topo da lista. Algumas colunas desnecessárias (como as características PCA e o próprio cluster) são removidas para limpar o resultado.\n",
    "\n",
    "6. **Retorno das recomendações**: A função retorna o conjunto de imóveis recomendados, ordenados pela sua similaridade com o imóvel procurado, com base na distância euclidiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElzCwJ3FYZM2",
    "outputId": "22e0b6b9-95f0-4459-8c5b-0a68ddfa6199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+------+----+------------------------+---------+\n",
      "|andar|area_util|quartos|suites|vaga|bairro                  |valor    |\n",
      "+-----+---------+-------+------+----+------------------------+---------+\n",
      "|0    |140.0    |3      |2     |2   |Recreio dos Bandeirantes|887000.0 |\n",
      "|0    |228.0    |3      |1     |2   |Barra da Tijuca         |1885000.0|\n",
      "|0    |228.0    |3      |1     |2   |Barra da Tijuca         |1882000.0|\n",
      "|0    |298.0    |2      |2     |1   |Barra da Tijuca         |4500000.0|\n",
      "|0    |115.0    |3      |2     |2   |Barra da Tijuca         |1498000.0|\n",
      "|0    |115.0    |3      |2     |2   |Barra da Tijuca         |1497000.0|\n",
      "|0    |148.0    |3      |1     |2   |Recreio dos Bandeirantes|1198000.0|\n",
      "|0    |148.0    |3      |1     |2   |Recreio dos Bandeirantes|1199000.0|\n",
      "|0    |148.0    |3      |1     |2   |Recreio dos Bandeirantes|1200000.0|\n",
      "|0    |140.0    |3      |1     |2   |Recreio dos Bandeirantes|888000.0 |\n",
      "+-----+---------+-------+------+----+------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recomenda_imovel('0034df72-124a-4383-a89f-a019850a2ba0', df_pronto)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nY7uQ6XatJvo",
    "o-UNG2-MQZr1",
    "9Pfo8w7XtUpx",
    "F7Tp0GJgMUjE",
    "laf88LSN8n99",
    "2O72woFN8sHt",
    "uOeVdCXSy6BR",
    "b7zdWD6ujdAa"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
